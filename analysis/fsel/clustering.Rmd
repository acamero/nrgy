---
output: pdf_document
---
```{r setup, echo=FALSE, warning=FALSE, message=FALSE}
rm(list = ls())
options(error=traceback)
set.seed(1)
library(clValid)
library(kohonen)
library(MASS)
library(class)
#library(NbClust)
```



# Agrupación basada en centros

Agrupamos los perfiles diarios de consumo de cada edificio en 3 grupos de manera independiente para cada edificio.

``` {r echo=FALSE, warning=FALSE}
# Convertir datos de entrada en una lista de matrices
data_2_lmat <- function(data, metadata) {
  matrix <- data[,!colnames(data) %in% c("file_name","consumption_date")]
  l_matrix <- lapply(1:nrow(metadata), function(s) {
    matrix[which(data$file_name == metadata$file_name[s]),]
  })
  return(l_matrix)
}

# Cargamos los datos de consumo
setwd("/home/andu/Documentos/uma/PHD/papers/vatia/results")
raw_data <- read.table("data.txt",sep="\t")
raw_meta <- read.table("meta.txt", sep="\t", encoding = "latin1")
building_names <- paste( toupper(substr(raw_meta$name, 1,1)), tolower(substr(raw_meta$name, 2,20)), sep="")


# Y los convertimos en una lista de matrices
list_matrix_con <- data_2_lmat(raw_data,raw_meta)
# Definimos el número de agrupaciones para los perfiles de consumo de cada edificio
num_clusters <- 3
# Agrupamos para cada edificio de manera independiente
set.seed(1)
clusters <- lapply(list_matrix_con, function(temp) {
  kmeans(temp, num_clusters, 30, 10)
})


```

Luego, utilizando los 3 centros de cada agrupación (para cada edificio), procedemos a agrupar los edificios. Concretamente, para cada edificio generamos un vector compuesto por los tres centros ordenados según el centro que representa a más perfiles de consumo.

``` {r echo=FALSE, warning=FALSE}
get_centers_vector <- function(cluster) {
  ord <- order(cluster$size, decreasing = TRUE)
  out <- sapply(ord, function(s) { cluster$centers[s,] })
  return( as.vector(out) )
}

centers_matrix <- t( sapply(clusters, get_centers_vector ) )
rownames(centers_matrix) <- building_names

# Validación interna
iv_centers <- clValid(centers_matrix, 2:10, clMethods=c("hierarchical", "kmeans", "som", "pam"), validation="internal")
os_centers <- optimalScores(iv_centers)

# nb_centers <- NbClust(centers_matrix, distance = "euclidean", min.nc = 2,  max.nc = 10, method = "complete", index ="dunn")
```

Utilizando los métodos de agrupación **Jerárquico**, **k-means**, **SOM** y **PAM**, y las métricas de validación interna **Conectividad**, **Dunn** y **Silhouette** se busca el número óptimo de agrupaciones (entre 2 y 10). A continuación se muestran los mejores resultados para las tres métricas.

``` {r echo=FALSE, warning=FALSE}
os_centers
```



# Agrupación basada en centro representativo

Para cada edificio se selecciona el centro (de una de las 3 agrupaciones) que incorpora un mayor número de perfiles diarios de consumo, al cual llamaremos el centro  representativo del edificio. Utilizando dicho centro se busca el número óptimo de agrupaciones (con los mismos criterios que el caso anterior).

``` {r echo=FALSE, warning=FALSE}
get_rep_center_vector <- function(cluster) {
  ord <- order(cluster$size, decreasing = TRUE)
  out <- cluster$centers[ord[1],]
  return( as.vector(out) )
}

rep_center_matrix <- t( sapply(clusters, get_rep_center_vector ) )
rownames(rep_center_matrix) <- building_names

# Validación interna
iv_rep_center <- clValid(rep_center_matrix, 2:10, clMethods=c("hierarchical", "kmeans", "som", "pam"), validation="internal")
os_rep_center <- optimalScores(iv_rep_center)

```

A continuación se muestran los resultados:

``` {r echo=FALSE, warning=FALSE}
os_rep_center
```




# Agrupación basada en atributos representativos

Utilizando la técnica propuesta para encontrar el subconjunto de atributos mínimo sin pérdida de información se plantea sintetizar la información de un edificio. Para ello se ejecuta 30 veces de manera independiente el algoritmo para cada edificio, obteniendo 30 subconjuntos de atributos. A partir de estas soluciones se construye el histograma de frecuencia de aparición de cada atributo en las soluciones.

``` {r echo=FALSE, warning=FALSE}
# Establecemos el número máximo de edificios a cargar
num_buildings <- 64
# Variables para almacenar resultados de SSGA
results_ssga <- list()

setwd("/home/andu/Documentos/uma/PHD/papers/vatia/results")
for(s in 1:num_buildings) {
  temp <- list()
  for(n in 1:30) {
    file <- paste(n,"results-ssga.RData",sep="-")
    load(paste(s,file,sep="/"))
    temp[[n]] <- results
  }
  results_ssga[[s]] <- temp
}

# limpiamos
rm(results)
rm(time_beginning)
rm(time_end)
rm(s)
rm(n)
rm(temp)

# Definimos una función que dada una lista de soluciones (para un mismo edificio)
# retorna una lista con la frecuencia de aparición de cada atributo
get_freq <- function(list_results) {
  matrix <- t( sapply(list_results, function(s){ s$best*1 }) )
  freq <- colSums(matrix)
  return(freq)
}

# Convertimos las soluciones en frecuencias
freqs <-  t( sapply( results_ssga, get_freq ) )
rownames(freqs) <- building_names
```

Por ejemplo, a continuación se muestra el histograma de frecuencia de los atributos escogidos para dos edificios en particular (*Centro de Salud Arroyo de la Miel* y *Museo Picasso*).

``` {r echo=FALSE, warning=FALSE}
list_hours <- colnames(raw_data)[!colnames(raw_data) %in% c("file_name","consumption_date")]
list_hours <- sapply(list_hours, function(s) { substr(s, 2, nchar(s))})
par(mfrow=c(1,2))
barplot(freqs[16,], names.arg = list_hours,  xlab="Hour", ylab="Frequency", main="Healthcare Center", ylim=c(0,30))
barplot(freqs[9,], names.arg = list_hours,  xlab="Hour", ylab="Frequency", main="Museum", ylim=c(0,30))
```

Utilizando los criterios previamente definidos buscamos el tamaño óptimo de agrupaciones. Los resultados obtenidos se muestran a continuación.

``` {r echo=FALSE, warning=FALSE}

# Validación interna 
iv_freqs <- clValid(freqs, 2:10, clMethods=c("hierarchical", "kmeans", "som", "pam"), validation="internal")
os_freqs <- optimalScores(iv_freqs)

#Connectivity 5.8440476 hierarchical        2
#Dunn         0.6670578       kmeans       10
#Silhouette   0.1608837 hierarchical        2
```

``` {r echo=FALSE, warning=FALSE}
os_freqs
```



# Comparación de resultados

El método propuesto basado en el subconjunto de atributos presenta los mejores resultados según las métricas de **Conectividad** y **Dunn**. En cambio, el método propuesto de utilizar el 
centro "representativo" obtiene el mejor resultado para la métrica **Silhouette**.

A continuación se muestran los dendogramas para las dos mejores propuestas. A simple vista se puede observar una gran diferencia en el balanceo entre el primero y el segundo dendograma. Por otra parte, es posible apreciar que el segundo dendograma (frecuencia de atributos) presenta una mejor agrupación en términos "semánticos", por ejemplo la mayoría de los hospitales se encuentran a poca distancia, lo cual también se verifica para los centros de salud, y para otros edificios de características similares.


``` {r echo=FALSE, warning=FALSE}
par(mfrow=c(1,1))
hc_centers <- hclust(d=dist(rep_center_matrix))
plot(hc_centers, hang = -1, ylab="Height", xlab="Building", main="Representative Center Dendogram", sub="", cex=0.7)
```

``` {r echo=FALSE, warning=FALSE}
hc_freqs <- hclust(d=dist(freqs))
plot(hc_freqs, hang = -1, ylab="Height", xlab="Building", main="Attribute Freq Dendogram", sub="", cex=0.7)
```


